from __future__ import annotations

import argparse
import json
import logging
import os
import re
import sys
import time
import unicodedata
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Iterable, List

from dotenv import load_dotenv
load_dotenv()

import pandas as pd
from openai import OpenAI

from .ocr_backend import OCRError, ocr_pages
from .match import DatabaseMatcher

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

JSON_EXTRACT_INSTRUCTIONS = """
ä»¥ä¸‹ã®OCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç…§æ˜ã‚«ã‚¿ãƒ­ã‚°ã®å“ç•ª/å‹ç•ªã«ãªã‚Šå¾—ã‚‹å€™è£œã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã€å“ç•ªã®å®šç¾©ã€‘
- å“ç•ªã¯ã€2æ–‡å­—ã¾ãŸã¯3æ–‡å­—ä»¥ä¸Šã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ã€‚
- ãã®å¾Œã«3ã€œ5æ¡ã®æ•°å­—ãŒç¶šãã¾ã™ã€‚
ã€ä¾‹ã€‘
- âœ… è©²å½“ï¼šXNDN1500SLK 
- âœ… è©²å½“ï¼šAB12345
- âŒ éè©²å½“ï¼š2025-10-07ï¼ˆæ•°å­—ã®ã¿ï¼‰
- âŒ éè©²å½“ï¼šABCï¼ˆæ•°å­—ãŒãªã„ï¼‰

ã€ã‚¿ã‚¹ã‚¯ã€‘
- å“ç•ªã«è¦‹ãˆã‚‹æ–‡å­—åˆ—ã‚’æŠ½å‡ºã—ã€ä»¥ä¸‹ã®é–¢æ•° `emit_items` ã‚’å‘¼ã³å‡ºã—ã¦è¿”ã—ã¦ãã ã•ã„ã€‚
- normalized ã¯ NFKC ã§å…¨è§’â†’åŠè§’ã€ç©ºç™½é™¤å»ã€ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆå¤§æ–‡å­—åŒ–ã‚’æ–½ã—ã¦ãã ã•ã„ã€‚
- ä¾‹ï¼š"NNF41030 LE9" -> {"hinban": "NNF41030 LE9", "normalized": "NNF41030LE9"}
- æ™®é€šã®å˜èªãƒ»ç´”ç²‹ãªæ•°å­—ã®ã¿ã®ä¸¦ã³ãƒ»æ„å‘³ä¸æ˜ãªçŸ­ã„æ–‡å­—åˆ—ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚
- OCRã«ã‚ˆã‚Šåˆ†æ–­ã•ã‚ŒãŸå ´åˆã¯çµåˆã—ã¦ãã ã•ã„ã€‚(ä¾‹: "NNF 41030" -> "NNF41030")
- å‡ºåŠ›ã¯é–¢æ•°å‘¼ã³å‡ºã—ã®ã¿ã¨ã—ã€èª¬æ˜æ–‡ã¯å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚
"""

REGEX_PATTERN = re.compile(r"[A-Z]{1,5}\d{2,6}[A-Z0-9]*")


@dataclass
class MatchResult:
    input_hinban: str
    normalized: str
    match_status: str
    score: float
    matched_hinban: str | None = None
    zaiku: str | None = None
    page: int | None = None
    confidence: float | None = None


def _normalize_candidate(value: str) -> str:
    text = unicodedata.normalize("NFKC", value)
    text = re.sub(r"\s+", "", text)
    return text.upper()


def _unique_items(items: Iterable[dict]) -> List[dict]:
    unique: dict[str, dict] = {}
    for item in items:
        hinban = str(item.get("hinban", "")).strip()
        normalized = _normalize_candidate(item.get("normalized", hinban))
        if not normalized:
            continue
        if normalized not in unique:
            unique[normalized] = {"hinban": hinban, "normalized": normalized}
    return list(unique.values())


def extract_hinbans_with_gpt(
    text: str,
    model: str,
    base_url: str,
    api_key: str,
    timeout: int,
) -> dict:
    """
    Returns:
        {"method": "gpt_tool" | "regex_fallback", "items": list[dict]}
    """
    client = OpenAI(base_url=base_url, api_key=api_key, timeout=timeout)
    truncated_text = text[:8000]
    user_prompt = f"{JSON_EXTRACT_INSTRUCTIONS}\nOCRãƒ†ã‚­ã‚¹ãƒˆ:\n```\n{truncated_text}\n```"

    messages = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯OCRå¾Œã®ç…§æ˜ã‚«ã‚¿ãƒ­ã‚°ã‹ã‚‰å“ç•ªã‚’æŠ½å‡ºã™ã‚‹æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆè§£æã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚",
        },
        {"role": "user", "content": user_prompt},
    ]

    tools = [
        {
            "type": "function",
            "function": {
                "name": "emit_items",
                "description": "æŠ½å‡ºã—ãŸå“ç•ªå€™è£œã‚’é…åˆ—ã¨ã—ã¦è¿”ã™ã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "items": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "hinban": {"type": "string"},
                                    "normalized": {"type": "string"},
                                },
                                "required": ["hinban", "normalized"],
                            },
                        }
                    },
                    "required": ["items"],
                },
            },
        }
    ]

    backoff = 1.5
    attempt = 0
    max_attempts = 3

    while attempt < max_attempts:
        attempt += 1
        try:
            response = client.chat.completions.create(
                model=model,
                temperature=0,
                tools=tools,
                tool_choice={"type": "function", "function": {"name": "emit_items"}},
                messages=messages,
            )

            choice = response.choices[0] if response.choices else None
            tool_calls = getattr(choice.message, "tool_calls", None) if choice else None

            if not tool_calls:
                logger.warning("ãƒ¢ãƒ‡ãƒ«ãŒé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¾ã›ã‚“ã§ã—ãŸã€‚æ­£è¦è¡¨ç¾ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                return {"method": "regex_fallback", "items": _fallback_regex_extraction(text)}

            # å–ç¬¬ä¸€å€‹ tool call
            tc = tool_calls[0]
            args_raw = tc.function.arguments or ""
            # ç•™ä¸€æ‰‹æ—¥å¿—ï¼Œå‰ 400 å­—ç¬¦
            logger.debug("tool.arguments (first 400): %s", args_raw[:400].replace("\n", " "))

            args = json.loads(args_raw)
            parsed = args.get("items", [])
            if not isinstance(parsed, list):
                logger.warning("é–¢æ•°å¼•æ•°ãŒé…åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ­£è¦è¡¨ç¾ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                return {"method": "regex_fallback", "items": _fallback_regex_extraction(text)}

            normalized_items = _unique_items(parsed)
            if not normalized_items:
                logger.info("ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªå“ç•ªãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚æ­£è¦è¡¨ç¾ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                return {"method": "regex_fallback", "items": _fallback_regex_extraction(text)}

            return {"method": "gpt_tool", "items": normalized_items}

        except Exception as exc:
            status_code = getattr(exc, "status_code", None) or getattr(exc, "status", None)
            if status_code in (429, 500, 502, 503, 504) and attempt < max_attempts:
                sleep_time = backoff ** attempt
                logger.warning("OpenAI API error (status=%s). Retrying in %.1f seconds...", status_code, sleep_time)
                time.sleep(sleep_time)
                continue
            logger.exception("OpenAI APIå‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ­£è¦è¡¨ç¾ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
            return {"method": "regex_fallback", "items": _fallback_regex_extraction(text)}

    return {"method": "regex_fallback", "items": _fallback_regex_extraction(text)}


def _fallback_regex_extraction(text: str) -> list[dict]:
    normalized_text = unicodedata.normalize("NFKC", text.upper())
    candidates = []
    for match in REGEX_PATTERN.finditer(normalized_text):
        token = match.group(0)
        normalized = _normalize_candidate(token)
        candidates.append({"hinban": token, "normalized": normalized})
    return _unique_items(candidates)


def _match_semantic_items(
    items: list[dict], matcher: DatabaseMatcher, fuzzy_threshold: float = 0.82
) -> list[MatchResult]:
    all_rows = list(matcher.hinban_map.values())
    results: list[MatchResult] = []
    for item in items:
        hinban = str(item.get("hinban", "")).strip()
        normalized = _normalize_candidate(item.get("normalized", hinban))
        if not normalized:
            continue

        status = "NONE"
        score = 0.0
        matched_hinban: str | None = None
        matched_zaiku: str | None = None

        # å®Œå…¨ä¸€è‡´
        row = matcher.hinban_map.get(normalized)
        if row:
            status = "EXACT"
            matched_hinban = row.hinban
            matched_zaiku = row.zaiku or None
            score = 1.0
        else:
            # å­ä¸²äº’å«
            for candidate in all_rows:
                if normalized and normalized in candidate.hinban:
                    status = "SUBSTR"
                    matched_hinban = candidate.hinban
                    matched_zaiku = candidate.zaiku or None
                    score = 0.9
                    break
                if candidate.hinban and candidate.hinban in normalized:
                    status = "SUBSTR"
                    matched_hinban = candidate.hinban
                    matched_zaiku = candidate.zaiku or None
                    score = 0.9
                    break

        # è§„æ ¼å…³é”®å­—å›æŸ¥
        if status == "NONE":
            retry_candidates = matcher.retry(normalized)
            if retry_candidates:
                status = "KIDOU"
                matched_hinban = retry_candidates[0]
                row2 = matcher.hinban_map.get(matched_hinban)
                matched_zaiku = (row2.zaiku if row2 else None)
                score = 0.88

        # æ¨¡ç³ŠåŒ¹é…
        if status == "NONE":
            best_ratio = 0.0
            best_row = None
            from difflib import SequenceMatcher
            for candidate in all_rows:
                ratio = SequenceMatcher(None, normalized, candidate.hinban).ratio()
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_row = candidate
            if best_row and best_ratio >= fuzzy_threshold:
                status = "FUZZY"
                matched_hinban = best_row.hinban
                matched_zaiku = best_row.zaiku or None
                score = round(best_ratio, 3)

        results.append(
            MatchResult(
                input_hinban=hinban,
                normalized=normalized,
                match_status=status,
                score=score,
                matched_hinban=matched_hinban,
                zaiku=matched_zaiku,
            )
        )
    return results


def process_pdf_semantic(
    pdf_path: str,
    db_path: str,
    model: str,
    base_url: str,
    api_key: str | None,
    timeout: int,
    save: bool,
) -> pd.DataFrame:
    resolved_base_url = (
        base_url
        or os.getenv("OPENAI_BASE_URL")
        or os.getenv("CUSTOM_OPENAI_BASE_URL")
        or "https://api.openai.com/v1"
    )
    resolved_api_key = (
        api_key
        or os.getenv("OPENAI_API_KEY")
        or os.getenv("CUSTOM_OPENAI_API_KEY")
    )
    if not resolved_api_key:
        message = "OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        logger.error(message)
        raise RuntimeError(message)

    pdf_file = Path(pdf_path)
    if not pdf_file.exists():
        message = f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_path}"
        logger.error(message)
        raise RuntimeError(message)

    db_file = Path(db_path)
    if not db_file.exists():
        message = f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}"
        logger.error(message)
        raise RuntimeError(message)

    # OCR
    try:
        texts = ocr_pages(str(pdf_file))
    except OCRError as exc:
        message = f"OCRå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}"
        logger.error(message)
        raise RuntimeError(message) from exc
    except Exception as exc:
        message = f"PDFã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}"
        logger.error(message)
        raise RuntimeError(message) from exc

    # OCRãƒ†ã‚­ã‚¹ãƒˆçµåˆ
    ocr_text = "\n".join(texts)

    # GPTæŠ½å‡ºï¼ˆé–¢æ•°å‘¼ã³å‡ºã—æ–¹å¼ï¼‰
    logger.info("ğŸ§  GPTã§å“ç•ªã‚’æŠ½å‡ºä¸­...")
    result = extract_hinbans_with_gpt(
        text=ocr_text,
        model=model,
        base_url=resolved_base_url,
        api_key=resolved_api_key,
        timeout=timeout,
    )
    method = result.get("method", "unknown")
    items = result.get("items", [])

    # æŠ½å‡ºçµæœã®è¡¨ç¤º + ä¿å­˜ï¼ˆmethod ä»˜ãï¼‰
    title = "ğŸ§  GPTæŠ½å‡ºçµæœ (Function Call)" if method == "gpt_tool" else "ğŸ§ª æ­£è¦è¡¨ç¾æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"
    print(f"\n=== {title} ===")
    if not items:
        print("âš ï¸ æŠ½å‡ºçµæœãŒç©ºã§ã™ã€‚")
    else:
        for i, item in enumerate(items, 1):
            print(f"{i:02d}. {item.get('hinban', '')}  â†’  {item.get('normalized', '')}")
    print("=" * 60)

    logs_dir = Path("logs")
    logs_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    gpt_csv_path = logs_dir / f"extract_{method}_{timestamp}.csv"
    pd.DataFrame(items).to_csv(gpt_csv_path, index=False, encoding="utf-8-sig")
    logger.info("ğŸ’¾ æŠ½å‡ºçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: %s", gpt_csv_path)

    # DBç…§åˆ
    try:
        matcher = DatabaseMatcher(db_file)
    except ValueError as exc:
        message = str(exc)
        logger.error(message)
        raise RuntimeError(message) from exc

    match_results = _match_semantic_items(items, matcher)

    # ç…§åˆçµæœã®è¡¨ç¤ºï¼ˆåœ¨åº«ä»˜ãï¼‰
    print("\n=== ğŸ” ç…§åˆçµæœ ===")
    for result in match_results:
        matched = result.matched_hinban or "-"
        z = result.zaiku or "-"
        print(f"{result.match_status:<6} | {result.input_hinban:<20} -> {matched:<20} | åœ¨åº«={z:<10} | score={result.score:0.3f}")

    # DataFrameï¼ˆåœ¨åº«åˆ—ä»˜ãï¼‰
    df = pd.DataFrame(
        [
            {
                "input_hinban": r.input_hinban,
                "normalized": r.normalized,
                "match_status": r.match_status,
                "score": r.score,
                "matched_hinban": r.matched_hinban,
                "zaiku": r.zaiku,
                "page": r.page,
                "confidence": r.confidence,
                "method": method,
            }
            for r in match_results
        ]
    )

    if save:
        output_path = logs_dir / f"match_{method}_{timestamp}.csv"
        df.to_csv(output_path, index=False, encoding="utf-8-sig")
        logger.info("ğŸ’¾ ç…§åˆçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: %s", output_path)

    return df


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Semantic matcher for hinban extraction")
    parser.add_argument("--pdf", required=True, help="å…¥åŠ›PDFã®ãƒ‘ã‚¹")
    parser.add_argument("--db", required=True, help="å“ç•ªCSVã®ãƒ‘ã‚¹")
    parser.add_argument("--model", default="gpt-4o-mini", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--base-url", default=None, help="OpenAIäº’æ›APIã®ãƒ™ãƒ¼ã‚¹URL")
    parser.add_argument("--api-key", default=None, help="OpenAIäº’æ›APIã®ã‚­ãƒ¼")
    parser.add_argument("--timeout", type=int, default=60, help="APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ(ç§’)")
    parser.add_argument("--save", action="store_true", help="çµæœã‚’CSVã«ä¿å­˜")
    return parser.parse_args()


if __name__ == "__main__":
    args = _parse_args()
    try:
        process_pdf_semantic(
            pdf_path=args.pdf,
            db_path=args.db,
            model=args.model,
            base_url=args.base_url,
            api_key=args.api_key,
            timeout=args.timeout,
            save=args.save,
        )
    except RuntimeError:
        sys.exit(1)
